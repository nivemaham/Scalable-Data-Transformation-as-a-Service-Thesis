% Chapter 2

\chapter{Literature Review} % Main chapter title
\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter2} 

\lhead{Chapter 2. \emph{Related work}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
\section{Escalated Requirements in Interactive Data Preparation}
\label{sec:requirements}
In this section, we discuss the important requirements which should be met by an integrated, interactive open data preparation solution. 
\subsection{Reusable and Repeatable cleaning}
\noindent A data transformation process can be performed on more than one data-set. Hence, it is important to have share-able data transformations between different data-sets, to save efforts spent on redundant data preparation process. Moreover, when a data cleaning is crowd-sourced, the need for capturing data alterations is increased\cite{2011-wrangler}.  Hence, the solution must keep track of how a data had been processed during the data cleaning process and should be share-able. 

\subsection{Sampling approaches of large volumes of data}
\noindent Processing large volumes of data takes significant amount of time, especially when it comes to iterative processes. This makes the interactivity between user actions and user feedback with user harder and more expensive to perform. For example, consider a scenario when a cleaning operation was performed wrongly and rolled-back. This leads to a significant waste of computing resources, especially if it was executed on large data. Big data processing requires rethinking traditional processes \cite{nemreport}. Correctness and efficiency are important attributes of data cleaning \cite{journals/corr/KrishnanW0FG16}. Sampling data-sets is a well-known technique of data cleaning and analysis \cite{Hellerstein08quantitativedata} for various purposes. This can be exploited in iterative cleaning of large data. Typically, Sequential sampling (fetching an ordered subset) and random sampling (fetch subset from randomized data) are used to sample raw data. However, improper sampling can lead to wrong decisions \cite{journals/corr/KrishnanW0FG16}. For example a quantitative data may not able to spot outliers correctly if it is sampled sequentially (e.g. subset of first-N rows and calculate the average of sample), whereas random sampling produced more accurate results for aforementioned scenario\cite{Hellerstein08quantitativedata}. Sampling mechanism can depend on data being sampled, thus, allowing the user to choose the sampling technique according to input data provides accuracy and flexibility. Currently, data cleaning applications such as Wisteria \cite{Wisteria} and KARMA  \cite{knoblock15:aimag} use sampling for interactive data cleaning. However, user is unknown and not given an option to decide on sampling mechanism. The proposed solution should provide a feasible mechanism to sample input data, and allow user to decide on sampling approach. 
\subsection{Flexibility}
\noindent Interactive data cleaning and transformations are often supported by underlying Domain Specific Languages (DSL) \cite{Wisteria}. They are typically less exposed and strongly typed. Although, the proposed system should support non-technical workers to do data cleaning, the optimal value can be achieved if the flexibility is given for customized transformation for technical users. 
\section{Related Works}
\label{sec:relatedwork}
Related works which fall under the categories of linked data and open data preparation and ETL tools are discussed in following section. The most relevant systems are discussed with distinctive features highlighted.
\subsection{OpenRefine}
OpenRefine is the most relevant tool regarding the functional requirements of this work, allows to load, understand, clean, reconcile and augment data\footnote{https://github.com/OpenRefine/OpenRefine/blob/master/README.md}. OpenRefine, which was originally GoogleRefine, provides support for both data cleaning and RDF mapping and transformation using RDF Refine plugin, with an interactive user interface. OpenRefine is developed in Java and Jetty that runs locally on host machine as a desktop application. It is inefficient with large data volumes since it implements multi-pass approach \cite{onestopshotforopendata} which consumes large amount of computing resources while executing. It doesn't scale with the size of data. In addition, being an offline desktop application it cannot support collaborative cleaning and transformation.  BatchRefine\footnote{https://github.com/fusepoolP3/p3-batchrefine} is a collection of wrappers that provides support to run OpenRefine in batch mode. However, it has some limitations such as should be executed from command-line, lack of features implemented to support distributed processing, complex installation. This introduces complexity of execution via scripts which required technical knowledge. Further, neither OpenRefine nor BatchRefine were developed with component based architecture, results in tight coupling of core components. This prevents users from reusing this implementation for newer requirements. The proposed system aim to overcome these problems by providing a single ended, large scale open data preparation tool as a service.  
\subsection{Trifacta's Wrangler}
Wrangler is a data cleaning software, provided as a limited desktop application that provides interactive user interfaces to preview data cleaning results in real time \cite{2011-wrangler} \cite{visualizationsandtransformationsinwrangling} \cite{Keim08visualanalytics:}, currently process up to maximum 100 MB data, provided by Trifacta. Wrangler Enterprise\footnote{https://www.trifacta.com/products/wrangler-enterprise/} is an expensive, commercialized application that can support data cleaning in large scale using pipelines supported by Spark or MapReduce. However, main impediment seen in Wranger is, it purely concentrates on data cleaning without any support for linked-data preparation. In addition, Wrangler focuses only on single data cleaning and requires user to manually write domain specific scripting to perform data cleaning which requires learning of related scripts. Further, Wrangler is a desktop application, doesn't support collaborative transformation, reusing or sharing of transformation. By providing a service based solution that can process data cleaning, transformation and integration on large data with interactive user interfaces, the proposed solution is expected to be more useful for open data workers. 
\subsection{KARMA}
KARMA\footnote{http://usc-isi-i2.github.io/karma/} is a data integration tool, built by University of Southern California, that helps mapping structured data into semantic web data, also called as linked data \cite{karma}. KARMA supports integration of data from various sources. It allows users to 1) import data from wide variety of data sources, 2) clean and normalize data, 3) quickly build a semantic description of data  4) integrate data using those semantic models \cite{knoblock15:aimag}. KARMA is one of the main inspiration of this work. However, KARMA focuses on schema level integration, but not record level integration. Specially, KARMA is not a cloud based application, which hinders execution on very large volume \cite{knoblock15:aimag} as well as KARMA focuses on volume by executing transformation on sampling, but doesn't concentrate on data velocity. The proposed system is a cloud based solution and aimed to model the high level cleaning and transformation engine independent from data velocity such that can easily support data streaming and micro batching if necessary. 
\section{Discussion}
A simple comparative analysis was done on aforementioned relevant systems to distinguish the importance of proposed system. Table \ref{tab:2} shows the comparison between those systems and proposed solution. This proves that proposed solution has significant relevance and resolves important issues in the domain. 
\begin{center}
	\includegraphics[width=38em]{./Figures/comparative_analysis}
	\begin{table}[htbp]
    \caption{Comparative analysis on relevant systems}
    \label{tab:2}
	\end{table}
\end{center}
