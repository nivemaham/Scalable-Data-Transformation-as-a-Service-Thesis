% Chapter 2

\chapter{Literature Review} % Main chapter title
\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter2} 

\lhead{Chapter 2. \emph{Related work}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
\section{Escalated Requirements in Domain}
\label{sec:requirements}
In this section, the important requirements identified which should be solved by an integrated open data preparation solution are discussed. 
\textbf{Reusable and Repeatable cleaning }
The same transformation process can be performed on multiple data-sets. Importantly, when a data cleaning is crowd-sourced, the need for capturing data alterations is increased\cite{2011-wrangler}.  Hence, the solution must keep track of how a data had been processed during the data cleaning process and should be share-able. 

\subsection{Real-time responsive architecture}
\noindent Interactive data transformation directly derives the need for responsive user interfaces. This is possible when data being processed is small. For example, OpenRefine \cite{openrefine}  is an interactive featured open data cleanser can process only small volumes of data, whereas the size of the data that need to be processed growing exponentially. The root cause for this issue is these systems are not considered to be effective on large volume of data during design and implementation state. It is of utmost important that the next generation solutions should be considered to process large volumes of data from designing stage itself. There is no disclosed architecture or an efficient and effective work-flow which can work with real-time interactive cleaning on large data.  An optimized process that should work for big data cleaning in real-time should be considered and implemented by proposed solution. 
\subsection{Sampling}
\noindent Processing a large data takes significant amount of time that may be accumulated to longer duration for the whole multi-step process. This will make the interaction harder and expensive. For example, considering a scenario when a cleaning operation was performed wrong and rolled-back, this leads to a significant waste of resources if it was executed on large data. Big data processing requires rethinking of traditional processes \cite{nemreport}. Correctness and efficiency are important attributes of data cleaning \cite{journals/corr/KrishnanW0FG16}. Sampling data-sets is a well-known technique of data cleaning and analysis \cite{Hellerstein08quantitativedata} for various purpose. This can be exploited in interactive and iterative data cleaning. Sequential sampling (fetching an ordered subset) and random sampling (fetch subset from randomized data) can help for large inputs. However, improper sampling can lead to wrong decisions \cite{journals/corr/KrishnanW0FG16}. For example a quantitative data may not able to spot outliers correctly if it is sampled sequentially (e.g. subset of first-N rows and calculate the average of sample). It will produce more accurate data if it is sampled randomly \cite{Hellerstein08quantitativedata}. There is a gap in the literature to pinpoint a proper mechanism to sample data for iterative data cleansing. A data cleaning platform Wisteria \cite{Wisteria} and data integration tool KARMA  \cite{knoblock15:aimag} use sampling for interactive data cleaning. But the sampling method is unknown to the user and user is not given a choice to choose sampling mechanism.  Thus, a feasible mechanism to decide on sampling should be available to the user.
\subsection{Availability and Accessibility}
\noindent Data preparation is not an instant process. A user should be able to perform consecutive transformations on a system for long duration. The system should not have a single point of failure \cite{mesa}.  In addition, in open data field, crowd-sourced data cleaning and transformation is getting popular which are called \textit{Multi-sector planning and analysis} \cite{multisectoranalysis}. It requires collaborative transformations, i.e. a data cleaning and transformation done by multiple partners. Pure offline applications hinders collaborative transformations. Cloud computing and distributed computing paradigm has proven solutions for availability issues. Providing solutions as a service enables more availability and waives complex environment installations and maintenance costs. This emphasis the need of an online data preparation application that can be shared-accessed and available between group of people.
\subsection{Separation between logical and physical implementations of data cleaning work-flow}
\noindent Most of the data cleaning tools don't have clear separation of their logical and physical implementation of data transformation activities \cite{declarativedatacleaning}\cite{Wisteria}. Typically ETL work-flows are defined in high-level languages \cite{ETL}. This ceases from exploiting the logical query optimization which substantially results in inefficient and high-cost transformation work-flow for bigger data.  Logical query optimization is not possible unless otherwise the data worker consciously designs the cleaning work-flow accordingly \cite{ETL}. An automated mechanism to optimize cleaning work-flow can improve performance of data cleaning. 
\subsection{Flexibility}
\noindent Interactive data cleaning and transformations are often supported by underlying Domain Specific Languages (DSL) \cite{Wisteria}. They are typically less exposed and strongly typed. Although, the proposed system should support non-technical workers to do data cleaning, the optimal value can be achieved if the flexibility is given for customized transformation for technical users. 
\section{Related Works}
\label{sec:relatedwork}
Related works which fall under the categories of linked data and open data preparation and ETL tools are discussed in following section. The most relevant systems are discussed with distinctive features highlighted.
\subsection{OpenRefine}
OpenRefine is the most relevant tool regarding the functional requirements of this work, allows to load, understand, clean, reconcile and augment data\footnote{https://github.com/OpenRefine/OpenRefine/blob/master/README.md}. OpenRefine, which was originally GoogleRefine, provides support for both data cleaning and RDF mapping and transformation using RDF Refine plugin, with an interactive user interface. OpenRefine is developed in Java and Jetty that runs locally on host machine as a desktop application. It is inefficient with large data volumes since it implements multi-pass approach \cite{onestopshotforopendata} which consumes large amount of computing resources while executing. It doesn't scale with the size of data. In addition, being an offline desktop application it cannot support collaborative cleaning and transformation.  BatchRefine\footnote{https://github.com/fusepoolP3/p3-batchrefine} is a collection of wrappers that provides support to run OpenRefine in batch mode. However, it has some limitations such as should be executed from command-line, lack of features implemented to support distributed processing, complex installation. This introduces complexity of execution via scripts which required technical knowledge. Further, neither OpenRefine nor BatchRefine were developed with component based architecture, results in tight coupling of core components. This prevents users from reusing this implementation for newer requirements. The proposed system aim to overcome these problems by providing a single ended, large scale open data preparation tool as a service.  
\subsection{Trifacta's Wrangler}
Wrangler is a data cleaning software, provided as a limited desktop application that provides interactive user interfaces to preview data cleaning results in real time \cite{2011-wrangler} \cite{visualizationsandtransformationsinwrangling} \cite{Keim08visualanalytics:}, currently process up to maximum 100 MB data, provided by Trifacta. Wrangler Enterprise\footnote{https://www.trifacta.com/products/wrangler-enterprise/} is an expensive, commercialized application that can support data cleaning in large scale using pipelines supported by Spark or MapReduce. However, main impediment seen in Wranger is, it purely concentrates on data cleaning without any support for linked-data preparation. In addition, Wrangler focuses only on single data cleaning and requires user to manually write domain specific scripting to perform data cleaning which requires learning of related scripts. Further, Wrangler is a desktop application, doesn't support collaborative transformation, reusing or sharing of transformation. By providing a service based solution that can process data cleaning, transformation and integration on large data with interactive user interfaces, the proposed solution is expected to be more useful for open data workers. 
\subsection{KARMA}
KARMA\footnote{http://usc-isi-i2.github.io/karma/} is a data integration tool, built by University of Southern California, that helps mapping structured data into semantic web data, also called as linked data \cite{karma}. KARMA supports integration of data from various sources. It allows users to 1) import data from wide variety of data sources, 2) clean and normalize data, 3) quickly build a semantic description of data  4) integrate data using those semantic models \cite{knoblock15:aimag}. KARMA is one of the main inspiration of this work. However, KARMA focuses on schema level integration, but not record level integration. Specially, KARMA is not a cloud based application, which hinders execution on very large volume \cite{knoblock15:aimag} as well as KARMA focuses on volume by executing transformation on sampling, but doesn't concentrate on data velocity. The proposed system is a cloud based solution and aimed to model the high level cleaning and transformation engine independent from data velocity such that can easily support data streaming and micro batching if necessary. 
\section{Discussion}
A simple comparative analysis was done on aforementioned relevant systems to distinguish the importance of proposed system. Table \ref{tab:2} shows the comparison between those systems and proposed solution. This proves that proposed solution has significant relevance and resolves important issues in the domain. 
\begin{center}
	\includegraphics[width=38em]{./Figures/comparative_analysis}
	\begin{table}[htbp]
    \caption{Comparative analysis on relevant systems}
    \label{tab:2}
	\end{table}
\end{center}
